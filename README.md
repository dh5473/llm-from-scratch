# LLM from Scratch

---

## ğŸ¯ ìŠ¤í„°ë”” ëª©í‘œ

1. ğŸ“‘ ë…¼ë¬¸ì˜ ì´ë¡ ê³¼ êµ¬í˜„ ì½”ë“œë¥¼ ì—°ê²°í•˜ì—¬ ê°œë…ì„ ìì„¸íˆ ì„¤ëª…í•˜ëŠ” ì½˜í…ì¸  ì‘ì„±
2. ğŸ§© êµ¬í˜„í•œ ì½”ë“œë¥¼ ì •ë¦¬í•˜ì—¬ LLM ì—°êµ¬ë¥¼ ìœ„í•œ ì‘ì€ íŒ¨í‚¤ì§€ êµ¬ì¶•
3. ğŸŒŸ (ì„ íƒ) HuggingFace Code Contribution

---

## ğŸ“† ì£¼ì°¨ë³„ ì»¤ë¦¬í˜ëŸ¼

<details>
<summary><b>Week 0: LLMì— ëŒ€í•œ ë„“ê³  ì–•ì€ ì§€ì‹</b></summary>
<br>
LLMì— ëŒ€í•œ ê¸°ì´ˆì ì¸ ì´í•´ì™€ ì „ë°˜ì ì¸ ê°œë… í•™ìŠµ
</details>

<details>
<summary><b>Week 1: Architecture and Generation</b></summary>
<br>

### ğŸ”‘ í‚¤ì›Œë“œ
Causal LM, Decoding Strategy, LLM Inference / Generation

### ğŸ“š í•™ìŠµ ë‚´ìš©
- ì „í˜•ì ì¸ ì˜¤í”ˆì†ŒìŠ¤ LLMì˜ ì•„í‚¤í…ì²˜ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤.
- ì›ë³¸ íŠ¸ëœìŠ¤í¬ë¨¸ ë””ì½”ë”ì™€ í˜„ëŒ€ LLMì˜ êµ¬ì„± ìš”ì†Œë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
- í† í¬ë‚˜ì´ì €ì™€ ì–¸ì–´ ëª¨ë¸ì˜ ê¸°ë³¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.

### ğŸ“– ì°¸ê³  ìë£Œ
- [Llama-3.2-1B](https://huggingface.co/meta-llama/Llama-3.2-1B)
- [Transformers Auto Classes](https://huggingface.co/docs/transformers/en/model_doc/auto)
- [Text Generation](https://huggingface.co/docs/transformers/main_classes/text_generation)
- [Generation Strategies](https://huggingface.co/docs/transformers/generation_strategies)

### ğŸ’» ì‹¤ìŠµ ê³¼ì œ
- `transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ `Auto` í´ë˜ìŠ¤ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
- `forward`ì™€ `generate` ë©”ì„œë“œì˜ ê´€ê³„ì™€ ì°¨ì´ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
- Forward Passì—ì„œ Activation Memoryê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì‚´í´ë´…ë‹ˆë‹¤.
- ê¸°ë³¸ì ì¸ ë””ì½”ë”© ì „ëµì„ ì‚´í´ë³´ê³ , ìƒ˜í”Œë§ì— ê´€ì—¬í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
</details>

<details>
<summary><b>Week 2: Embedding and RAG</b></summary>
<br>

### ğŸ”‘ í‚¤ì›Œë“œ
Embedding, Attention, RAG

### ğŸ“š í•™ìŠµ ë‚´ìš©
- ì–¸ì–´ ëª¨ë¸ì˜ ì„ë² ë”© ë ˆì´ì–´ì™€ ì„ë² ë”© ëª¨ë¸ì˜ ì°¨ì´ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤.
- RAGì™€ ê´€ë ¨ëœ ê¸°ë³¸ì ì¸ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.

### ğŸ“– ì°¸ê³  ìë£Œ
- [Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks](https://arxiv.org/abs/2005.11401)
- [REALM: Retrieval-Augmented Language Model Pre-Training](https://arxiv.org/abs/1908.10084)
- [Getting Started with Embeddings](https://huggingface.co/blog/getting-started-with-embeddings)
- [Kanana Nano 2.1b Embedding](https://huggingface.co/kakaocorp/kanana-nano-2.1b-embedding)
- [Transformers RAG Documentation](https://huggingface.co/docs/transformers/en/model_doc/rag)

### ğŸ’» ì‹¤ìŠµ ê³¼ì œ
- `forward` ë©”ì„œë“œì˜ ì‹¤í–‰ ê³¼ì •ì—ì„œ Hidden Stateì™€ Attention Mapì„ ì¶”ì¶œí•˜ëŠ” ë°©ë²•ì„ ì´í•´í•©ë‹ˆë‹¤.
- `sentence_transformers` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìˆ˜í–‰í•˜ê³ , ê°„ë‹¨í•œ ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰ì„ ì‹¤ìŠµí•©ë‹ˆë‹¤.
</details>

<details>
<summary><b>Week 3: Fine Tuning LLMs</b></summary>
<br>

### ğŸ”‘ í‚¤ì›Œë“œ
Fine-Tuning, Supervised Fine Tuning, Chat Template

### ğŸ“š í•™ìŠµ ë‚´ìš©
- LLMì˜ í•™ìŠµ ë° ì¶”ë¡  ì›ë¦¬ë¥¼ ì´í•´í•˜ê³ , íŒŒì¸íŠœë‹ ê³¼ì •ì„ ì‚´í´ë´…ë‹ˆë‹¤.
- Baseì™€ Chat ëª¨ë¸ì˜ ì°¨ì´ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
- HuggingFaceì—ì„œ ë°ì´í„°ì…‹ì„ ë¶ˆëŸ¬ì˜¤ê³  ì¡°ì‘í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´…ë‹ˆë‹¤.
- Parameter Efficient Fine Tuning(PEFT)ì˜ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.

### ğŸ“– ì°¸ê³  ìë£Œ
- [LIMA: Less Is More for Alignment](https://arxiv.org/abs/2305.11206)
- [Instruction Tuning with GPT-4](https://arxiv.org/abs/2304.03277)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [Hugging Face Datasets](https://huggingface.co/docs/datasets/index)
- [NLP Processing with Datasets](https://huggingface.co/docs/datasets/nlp_process)
- [PEFT Documentation](https://huggingface.co/docs/peft/index)
- [LoRA Developer Guide](https://huggingface.co/docs/peft/developer_guides/lora)
- [Chat Templating](https://huggingface.co/docs/transformers/main/chat_templating)
- [Meta Llama 3 Prompt Formats](https://www.llama.com/docs/model-cards-and-prompt-formats/meta-llama-3/)

### ğŸ’» ì‹¤ìŠµ ê³¼ì œ
- [Base Model](https://huggingface.co/meta-llama/Llama-3.2-1B)ê³¼ [Instruct Model](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct)ì˜ ì°¨ì´ë¥¼ ì‚´í´ë´…ë‹ˆë‹¤.
- LLMì—ì„œ ì‚¬ìš©ë˜ëŠ” Special Tokenì˜ ì—­í• ì„ ì´í•´í•©ë‹ˆë‹¤.
- `chat_template`ì„ ì‚¬ìš©í•˜ì—¬ LLM í•™ìŠµ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
- `peft` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒŒë¼ë¯¸í„° íš¨ìœ¨ì ì¸ íŒŒì¸ íŠœë‹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- `trainer` í´ë˜ìŠ¤ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
</details>

<details>
<summary><b>Week 4: LLM Quantization</b></summary>
<br>

### ğŸ”‘ í‚¤ì›Œë“œ
Quantization, Post Training Quantization, Quantization Aware Training

### ğŸ“š í•™ìŠµ ë‚´ìš©
- LLM Compressionì˜ ì—¬ëŸ¬ ê¸°ë²•ì„ ì‚´í´ë´…ë‹ˆë‹¤.
- LLM ì–‘ìí™” ê³¼ì •ì„ ì‚´í´ë´…ë‹ˆë‹¤.
- PTQì™€ QATì˜ ì°¨ì´ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
- Calibrationì˜ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.

### ğŸ“– ì°¸ê³  ìë£Œ
- [GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)
- [AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration](https://arxiv.org/abs/2306.00978)
- [Quantization in Hugging Face](https://huggingface.co/blog/merve/quantization)
- [BitsAndBytesConfig Documentation](https://huggingface.co/docs/transformers/main_classes/quantization#transformers.BitsAndBytesConfig)

### ğŸ’» ì‹¤ìŠµ ê³¼ì œ
- `BitsAndBytesConfig`ë¥¼ ì‚¬ìš©í•œ LLM ì–‘ìí™” ê³¼ì •ì„ ì´í•´í•©ë‹ˆë‹¤.
- Calibration ê³¼ì •ì„ ì‚´í´ë´…ë‹ˆë‹¤.
</details>

<details>
<summary><b>Week 5: Reinforcement Learning</b></summary>
<br>

### ğŸ”‘ í‚¤ì›Œë“œ
Reinforcement Learning, RLHF, Preference Optimization

### ğŸ“š í•™ìŠµ ë‚´ìš©
- LLMì„ í™œìš©í•œ ê°•í™”í•™ìŠµì´ ì–´ë–»ê²Œ ìˆ˜í–‰ë˜ëŠ”ì§€ ì‚´í´ë´…ë‹ˆë‹¤.
- PPO, RLHFì˜ ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.
- Preference Optimization ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.
- ë©”ëª¨ë¦¬ ê´€ì ì—ì„œ LLM RLì´ ì–´ë ¤ìš´ ì´ìœ ì™€ ê·¹ë³µ ë°©ì•ˆì— ëŒ€í•´ ì‚´í´ë´…ë‹ˆë‹¤.

### ğŸ“– ì°¸ê³  ìë£Œ
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
- [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)
- [TRL Documentation](https://huggingface.co/docs/trl/index)
- [PPO Trainer](https://huggingface.co/docs/trl/main/en/ppo_trainer)
- [DPO Trainer](https://huggingface.co/docs/trl/main/dpo_trainer)

### ğŸ’» ì‹¤ìŠµ ê³¼ì œ
ì´ë²ˆ ì£¼ì°¨ ì‹¤ìŠµì€ ë†’ì€ ë©”ëª¨ë¦¬ì˜ GPUë¥¼ í•„ìš”ë¡œ í•˜ë¯€ë¡œ, ì„ íƒì ìœ¼ë¡œ ì‹¤ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- [LLM RL](https://github.com/huggingface/trl/blob/main/trl/models/modeling_value_head.py)ì˜ êµ¬í˜„ ë°©ì‹ì„ ì´í•´í•©ë‹ˆë‹¤.
- `PPOTrainer`, `DPOTrainer`ë¥¼ ì‚¬ìš©í•œ LLM í•™ìŠµì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- PPO, DPO ê°ê°ì— ì‚¬ìš©í•˜ëŠ” Reward Dataset, Preference Datasetì„ ë¶„ì„í•©ë‹ˆë‹¤.
</details>

<details>
<summary><b>Week 6: Test Time Scaling</b></summary>
<br>

### ğŸ”‘ í‚¤ì›Œë“œ
Test Time Compute Scaling, Large Reasoning Model

### ğŸ“š í•™ìŠµ ë‚´ìš©
- Chain of Thought ë°©ë²•ë¡ ì„ ì´í•´í•©ë‹ˆë‹¤.
- LLM Reasoningê³¼ Test Time Compute Scalingì— ê³µë¶€í•©ë‹ˆë‹¤.
- Reward Modelì„ ì‚¬ìš©í•œ ìƒì„± ê³¼ì •ì„ ì´í•´í•©ë‹ˆë‹¤.

### ğŸ“– ì°¸ê³  ìë£Œ
- [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903)
- [System 2 Attention (S2A)](https://arxiv.org/abs/2303.11569)
- [Search and Learn](https://huggingface.co/learn/cookbook/en/search_and_learn)

### ğŸ’» ì‹¤ìŠµ ê³¼ì œ
- ë‹¤ì–‘í•œ í”„ë¡¬í”„íŒ… ë°©ë²•ë¡ ì„ ì‚¬ìš©í•´ì„œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ê³ , ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
- Best-of-N, Beam Search ë“±ì˜ Test Time Compute Scalingì„ ì§ì ‘ ìˆ˜í–‰í•©ë‹ˆë‹¤.
</details>

<details>
<summary><b>Week 7: Long Context</b></summary>
<br>

### ğŸ”‘ í‚¤ì›Œë“œ
Long Context LLM, RoPE, Needle in a Haystack

### ğŸ“š í•™ìŠµ ë‚´ìš©
- Attentionì˜ ë™ì‘ ì›ë¦¬ë¥¼ ë³µìŠµí•˜ê³ , LLMì˜ Context Length ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.
- LLMì˜ Context Lengthë¥¼ ëŠ˜ë¦¬ê¸° ìœ„í•œ ë°©ë²•ë¡ ì„ ì‚´í´ë´…ë‹ˆë‹¤.
- Long Context LLMì˜ ë¬¸ì œì ê³¼ ê·¹ë³µ ë°©ì•ˆì„ ê³µë¶€í•©ë‹ˆë‹¤.

### ğŸ“– ì°¸ê³  ìë£Œ
- [RoFormer: Enhanced Transformer with Rotary Position Embedding](https://arxiv.org/abs/2104.09864)
- [Awesome LLM Long Context Modeling](https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling)

### ğŸ’» ì‹¤ìŠµ ê³¼ì œ
- Long Contextì™€ ê´€ë ¨ëœ ì—°êµ¬ë¥¼ ì¡°ì‚¬í•œ í›„, ì–´ë–»ê²Œ êµ¬í˜„ë˜ì—ˆëŠ”ì§€ ì‚´í´ë´…ë‹ˆë‹¤.
- Long Context LLMì—ì„œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ(Needle in a Haystack)ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
</details>

<details>
<summary><b>Week 8: Efficient Inference</b></summary>
<br>

### ğŸ”‘ í‚¤ì›Œë“œ
KV Caching, Prompt Compression, Speculative Decoding

### ğŸ“š í•™ìŠµ ë‚´ìš©
- ì¶”ë¡  ë‹¨ê³„ì—ì„œ Transformerì˜ íš¨ìœ¨ì„±ì„ ê°œì„ í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì‚´í´ë´…ë‹ˆë‹¤.
- KV Caching ê°œë…ì„ ì´í•´í•©ë‹ˆë‹¤.
- Test Time Compute Scaling ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê¸°ë²•ì„ ì¡°ì‚¬í•©ë‹ˆë‹¤.

### ğŸ“– ì°¸ê³  ìë£Œ
- [Medusa: Simple LLM Inference Acceleration Framework](https://arxiv.org/abs/2311.10732)
- [SpecInfer: Accelerating Generative Large Language Model Serving with Speculative Inference](https://arxiv.org/abs/2305.09781)
- [DeepSpeed: Accelerating Large-scale Model Inference](https://arxiv.org/abs/2211.05102)
- [KV Cache Documentation](https://huggingface.co/docs/transformers/en/kv_cache)
- [Understanding KV Caching](https://huggingface.co/blog/not-lain/kv-caching)
- [Speculative Generation](https://huggingface.co/docs/text-generation-inference/conceptual/speculation)
- [Generation Strategies](https://huggingface.co/docs/transformers/en/generation_strategies)

### ğŸ’» ì‹¤ìŠµ ê³¼ì œ
- KV Caching ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ Inference ê³¼ì •ì˜ ì°¨ì´ë¥¼ ì´í•´í•©ë‹ˆë‹¤.
- Efficient Inference ê¸°ë²•ì˜ êµ¬í˜„ ë°©ë²•ì„ ì‚´í´ë´…ë‹ˆë‹¤.
</details>
