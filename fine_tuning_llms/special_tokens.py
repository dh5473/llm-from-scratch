from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd


def explore_special_tokens():
    """
    LLMì—ì„œ ì‚¬ìš©ë˜ëŠ” ìŠ¤í˜ì…œ í† í°ì˜ ì—­í• ì„ íƒìƒ‰í•©ë‹ˆë‹¤.
    ìŠ¤í˜ì…œ í† í°ì€ ëª¨ë¸ì—ê²Œ íŠ¹ì • ë¬¸ë§¥ì´ë‚˜ ì§€ì‹œì‚¬í•­ì„ ì „ë‹¬í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.
    ì˜ˆ) <|user|>, <|assistant|>, <|system|>, <s>, </s>, <|endoftext|> ë“±
    """
    model_name = "meta-llama/Llama-3.2-1B-Instruct"

    print(f"â³ {model_name}ì˜ ìŠ¤í˜ì…œ í† í°ì„ ë¶„ì„í•©ë‹ˆë‹¤...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    # ê¸°ë³¸ ìŠ¤í˜ì…œ í† í° ë¶„ì„
    print("\nğŸ“‹ ê¸°ë³¸ ìŠ¤í˜ì…œ í† í°:")

    special_tokens = {
        "BOS (Beginning of String)": tokenizer.bos_token,
        "EOS (End of String)": tokenizer.eos_token,
        "UNK (Unknown)": tokenizer.unk_token,
        "PAD (Padding)": tokenizer.pad_token,
        "SEP (Separator)": (
            tokenizer.sep_token if hasattr(tokenizer, "sep_token") else "ì—†ìŒ"
        ),
        "CLS (Classification)": (
            tokenizer.cls_token if hasattr(tokenizer, "cls_token") else "ì—†ìŒ"
        ),
        "MASK": tokenizer.mask_token if hasattr(tokenizer, "mask_token") else "ì—†ìŒ",
    }

    # ìŠ¤í˜ì…œ í† í° ID ì¶œë ¥
    tokens_info = []
    for name, token in special_tokens.items():
        token_id = tokenizer.convert_tokens_to_ids(token) if token != "ì—†ìŒ" else "ì—†ìŒ"
        print(f"  - {name}: '{token}' (ID: {token_id})")
        tokens_info.append({"í† í° ìœ í˜•": name, "í† í°": token, "í† í° ID": token_id})

    # í† í° ì¶”ê°€ í…ŒìŠ¤íŠ¸
    print("\nğŸ§ª ìƒˆë¡œìš´ ìŠ¤í˜ì…œ í† í° ì¶”ê°€ í…ŒìŠ¤íŠ¸:")

    # ê¸°ì¡´ ì–´íœ˜ í¬ê¸° í™•ì¸
    original_vocab_size = len(tokenizer)
    print(f"  - ì›ë˜ ì–´íœ˜ í¬ê¸°: {original_vocab_size}")

    # ìƒˆë¡œìš´ ìŠ¤í˜ì…œ í† í° ì¶”ê°€
    new_tokens = ["<|system|>", "<|user|>", "<|assistant|>", "<|thinking|>"]
    num_added = tokenizer.add_special_tokens({"additional_special_tokens": new_tokens})

    # ì–´íœ˜ í¬ê¸° ë³€í™” í™•ì¸
    print(f"  - ì¶”ê°€ëœ í† í° ìˆ˜: {num_added}")
    print(f"  - ìƒˆë¡œìš´ ì–´íœ˜ í¬ê¸°: {len(tokenizer)}")

    # ìƒˆë¡œ ì¶”ê°€ëœ í† í° ID í™•ì¸
    print("\nğŸ” ìƒˆë¡œ ì¶”ê°€ëœ ìŠ¤í˜ì…œ í† í° ID:")
    for token in new_tokens:
        token_id = tokenizer.convert_tokens_to_ids(token)
        print(f"  - '{token}' â†’ ID: {token_id}")
        tokens_info.append(
            {"í† í° ìœ í˜•": "ì¶”ê°€ëœ í† í°", "í† í°": token, "í† í° ID": token_id}
        )

    # ì˜ˆì‹œ ë¬¸ì¥ í† í°í™” í…ŒìŠ¤íŠ¸
    print("\nğŸ”¤ ìŠ¤í˜ì…œ í† í°ì„ í¬í•¨í•œ í…ìŠ¤íŠ¸ í† í°í™” ì˜ˆì‹œ:")

    test_texts = [
        "ì¼ë°˜ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
        f"{tokenizer.bos_token}ì‹œì‘ í† í°ì´ ìˆëŠ” í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤.{tokenizer.eos_token}",
        f"<|system|>\nì§€ì‹œì— ë”°ë¼ ëŒ€ë‹µí•˜ì„¸ìš”.\n<|user|>\ní•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?\n<|assistant|>\nì„œìš¸ì…ë‹ˆë‹¤.",
    ]

    for i, text in enumerate(test_texts):
        # í† í°í™”
        tokens = tokenizer.tokenize(text)
        token_ids = tokenizer.encode(text)

        print(f"\nì˜ˆì‹œ {i+1}: {text}")
        print(f"  - í† í° ìˆ˜: {len(tokens)}")
        print(
            f"  - í† í°: {tokens[:10]}..." if len(tokens) > 10 else f"  - í† í°: {tokens}"
        )
        print(
            f"  - í† í° ID: {token_ids[:10]}..."
            if len(token_ids) > 10
            else f"  - í† í° ID: {token_ids}"
        )

    # ìŠ¤í˜ì…œ í† í°ì˜ ì„ë² ë”© ì˜í–¥ ì„¤ëª…
    print("\nğŸ“š ìŠ¤í˜ì…œ í† í°ì˜ ì¤‘ìš”ì„±:")
    print("  1. ëª¨ë¸ ì…ì¶œë ¥ ê²½ê³„ í‘œì‹œ: BOS/EOSëŠ” í…ìŠ¤íŠ¸ì˜ ì‹œì‘ê³¼ ëì„ í‘œì‹œ")
    print("  2. ëŒ€í™” êµ¬ì¡°í™”: <|user|>, <|assistant|>ëŠ” ëŒ€í™” ì°¸ì—¬ì êµ¬ë¶„")
    print("  3. íŒŒì¸íŠœë‹ ì„±ëŠ¥ í–¥ìƒ: ëª…í™•í•œ ì—­í•  êµ¬ë¶„ìœ¼ë¡œ ëª¨ë¸ ì‘ë‹µ í’ˆì§ˆ ê°œì„ ")
    print(
        "  4. í† í¬ë‚˜ì´ì € ë³€ê²½ ì‹œ ì£¼ì˜: ìŠ¤í˜ì…œ í† í°ì„ ì¶”ê°€í•˜ë©´ ëª¨ë¸ ì„ë² ë”©ë„ ì¡°ì • í•„ìš”"
    )

    # ê²°ê³¼ ì €ì¥
    df = pd.DataFrame(tokens_info)
    output_path = "special_tokens_analysis.csv"
    df.to_csv(output_path, index=False)
    print(f"\nâœ… ìŠ¤í˜ì…œ í† í° ë¶„ì„ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    explore_special_tokens()
