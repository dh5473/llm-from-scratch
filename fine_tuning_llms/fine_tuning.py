import os
import torch
import numpy as np
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    HfArgumentParser,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from datasets import load_dataset, Dataset
import argparse
from dataclasses import dataclass, field
from typing import Optional, List


@dataclass
class FinetuningArguments:
    """
    íŒŒì¸íŠœë‹ì— í•„ìš”í•œ ì¸ìë“¤
    """

    model_name: str = field(
        default="meta-llama/Llama-3.2-1B-Instruct",
        metadata={"help": "ì‚¬ìš©í•  ê¸°ë³¸ ëª¨ë¸ì˜ ì´ë¦„ ë˜ëŠ” ê²½ë¡œ"},
    )
    dataset_name: str = field(
        default=None, metadata={"help": "ì‚¬ìš©í•  HuggingFace ë°ì´í„°ì…‹ ì´ë¦„"}
    )
    dataset_path: str = field(
        default=None, metadata={"help": "ìì²´ ë°ì´í„°ì…‹ì˜ ë¡œì»¬ ê²½ë¡œ"}
    )
    lora_r: int = field(default=8, metadata={"help": "LoRAì˜ ë­í¬"})
    lora_alpha: int = field(default=32, metadata={"help": "LoRA alpha íŒŒë¼ë¯¸í„°"})
    lora_dropout: float = field(default=0.05, metadata={"help": "LoRA ë“œë¡­ì•„ì›ƒ ë¹„ìœ¨"})
    output_dir: str = field(
        default="./results", metadata={"help": "ê²°ê³¼ë¬¼ì„ ì €ì¥í•  ë””ë ‰í† ë¦¬"}
    )
    num_train_epochs: int = field(default=1, metadata={"help": "í•™ìŠµ ì—í¬í¬ ìˆ˜"})
    per_device_train_batch_size: int = field(
        default=1, metadata={"help": "ë””ë°”ì´ìŠ¤ë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°"}
    )
    learning_rate: float = field(default=2e-5, metadata={"help": "í•™ìŠµë¥ "})
    max_seq_length: int = field(default=512, metadata={"help": "ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´"})
    load_in_8bit: bool = field(
        default=False, metadata={"help": "ëª¨ë¸ì„ 8ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë¡œë“œ"}
    )
    load_in_4bit: bool = field(
        default=False, metadata={"help": "ëª¨ë¸ì„ 4ë¹„íŠ¸ ì •ë°€ë„ë¡œ ë¡œë“œ"}
    )


def create_sample_dataset():
    """
    ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±
    """
    # ê°„ë‹¨í•œ ìƒ˜í”Œ ë°ì´í„°ì…‹ ì¤€ë¹„
    conversations = [
        {
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                },
                {"role": "user", "content": "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ íŒŒì¸íŠœë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"},
                {
                    "role": "assistant",
                    "content": "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ íŒŒì¸íŠœë‹ì€ ì‚¬ì „ í•™ìŠµëœ ëŒ€ê·œëª¨ ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì´ë‚˜ ë„ë©”ì¸ì— ë§ê²Œ ì¡°ì •í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì „ì²´ ëª¨ë¸ì„ ì²˜ìŒë¶€í„° í•™ìŠµì‹œí‚¤ëŠ” ê²ƒë³´ë‹¤ íš¨ìœ¨ì ì´ë©°, ì ì€ ì–‘ì˜ ë°ì´í„°ë¡œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                },
            ]
        },
        {
            "messages": [
                {
                    "role": "system",
                    "content": "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                },
                {
                    "role": "user",
                    "content": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
                },
                {
                    "role": "assistant",
                    "content": "íŒŒì´ì¬ì—ì„œ ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì£¼ìš” ì°¨ì´ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: 1) ê°€ë³€ì„±: ë¦¬ìŠ¤íŠ¸ëŠ” ê°€ë³€(mutable)ì´ê³  íŠœí”Œì€ ë¶ˆë³€(immutable)ì…ë‹ˆë‹¤. 2) ë¬¸ë²•: ë¦¬ìŠ¤íŠ¸ëŠ” ëŒ€ê´„í˜¸ [], íŠœí”Œì€ ì†Œê´„í˜¸ ()ë¡œ ì •ì˜í•©ë‹ˆë‹¤. 3) ì‚¬ìš© ìš©ë„: ë¦¬ìŠ¤íŠ¸ëŠ” ë™ì¼í•œ ìœ í˜•ì˜ í•­ëª© ì»¬ë ‰ì…˜ì— ì í•©í•˜ê³ , íŠœí”Œì€ ê´€ë ¨ëœ ë‹¤ë¥¸ ìœ í˜•ì˜ í•­ëª©ì„ ê·¸ë£¹í™”í•˜ëŠ” ë° ì í•©í•©ë‹ˆë‹¤.",
                },
            ]
        },
    ]

    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = {"train": conversations}
    return dataset


def preprocess_data(examples, tokenizer, max_seq_length):
    """
    ë°ì´í„° ì „ì²˜ë¦¬ í•¨ìˆ˜
    """
    # í† í¬ë‚˜ì´ì €ì˜ chat_templateì„ ì‚¬ìš©í•˜ì—¬ ë©”ì‹œì§€ í¬ë§·íŒ…
    formatted_texts = []

    for conversation in examples:
        formatted = tokenizer.apply_chat_template(
            conversation["messages"], tokenize=False, add_generation_prompt=True
        )
        formatted_texts.append(formatted)

    # ë°ì´í„°ì…‹ ìƒì„±
    dataset_dict = {"text": formatted_texts}
    dataset = Dataset.from_dict(dataset_dict)

    # í† í°í™” í•¨ìˆ˜ ì •ì˜
    def tokenize_function(examples):
        result = tokenizer(
            examples["text"],
            padding="max_length",
            truncation=True,
            max_length=max_seq_length,
            return_tensors=None,  # ê°œë³„ ìƒ˜í”Œì— í…ì„œë¥¼ ì ìš©í•˜ì§€ ì•ŠìŒ
        )
        result["labels"] = result["input_ids"].copy()
        return result

    # ë°ì´í„°ì…‹ì— í† í°í™” í•¨ìˆ˜ ì ìš©
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        remove_columns=["text"],
    )

    return tokenized_dataset


def fine_tune_model(args):
    """
    PEFTë¥¼ ì‚¬ìš©í•˜ì—¬ LLaMA ëª¨ë¸ íŒŒì¸íŠœë‹
    """
    print(f"â³ {args.model_name} ëª¨ë¸ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

    # í† í¬ë‚˜ì´ì € ë¡œë“œ
    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # ëª¨ë¸ ë¡œë“œ
    print("\nğŸ”„ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘...")
    load_options = {}

    if args.load_in_8bit:
        load_options["load_in_8bit"] = True
        print("  - 8ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©")
    elif args.load_in_4bit:
        load_options["load_in_4bit"] = True
        print("  - 4ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©")
    else:
        load_options["torch_dtype"] = torch.float16
        print("  - float16 ì •ë°€ë„ ì‚¬ìš©")

    model = AutoModelForCausalLM.from_pretrained(
        args.model_name, device_map="auto", **load_options
    )

    # ì–‘ìí™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ëª¨ë¸ ì¤€ë¹„
    if args.load_in_8bit or args.load_in_4bit:
        model = prepare_model_for_kbit_training(model)

    # LoRA ì„¤ì •
    print("\nğŸ”§ LoRA ì„¤ì • êµ¬ì„±...")
    peft_config = LoraConfig(
        r=args.lora_r,
        lora_alpha=args.lora_alpha,
        lora_dropout=args.lora_dropout,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=[
            "q_proj",
            "k_proj",
            "v_proj",
            "o_proj",
            "gate_proj",
            "up_proj",
            "down_proj",
        ],
    )

    print(f"  - LoRA ë­í¬: {args.lora_r}")
    print(f"  - LoRA ì•ŒíŒŒ: {args.lora_alpha}")
    print(f"  - LoRA ë“œë¡­ì•„ì›ƒ: {args.lora_dropout}")

    # PEFT ëª¨ë¸ ì¤€ë¹„
    model = get_peft_model(model, peft_config)
    model.print_trainable_parameters()

    # ë°ì´í„°ì…‹ ë¡œë“œ
    print("\nğŸ“š ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬...")
    if args.dataset_name:
        dataset = load_dataset(args.dataset_name)
        print(f"  - HuggingFace ë°ì´í„°ì…‹ '{args.dataset_name}'ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    elif args.dataset_path:
        dataset = load_dataset("json", data_files=args.dataset_path)
        print(f"  - ë¡œì»¬ ë°ì´í„°ì…‹ '{args.dataset_path}'ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    else:
        print("  - ë°ì´í„°ì…‹ ê²½ë¡œê°€ ì§€ì •ë˜ì§€ ì•Šì•„ ìƒ˜í”Œ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤.")
        dataset = create_sample_dataset()

    # ë°ì´í„° ì „ì²˜ë¦¬
    tokenized_dataset = preprocess_data(
        dataset["train"], tokenizer, args.max_seq_length
    )

    print(f"  - ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset['train'])} ìƒ˜í”Œ")
    print(f"  - ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´: {args.max_seq_length} í† í°")

    # ë°ì´í„° ì½œë ˆì´í„° ì„¤ì •
    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

    # í•™ìŠµ ì¸ì ì„¤ì •
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_train_epochs,
        per_device_train_batch_size=args.per_device_train_batch_size,
        gradient_accumulation_steps=4,
        learning_rate=args.learning_rate,
        weight_decay=0.01,
        warmup_ratio=0.03,
        lr_scheduler_type="cosine",
        logging_steps=1,
        save_steps=100,
        save_total_limit=3,
        fp16=torch.cuda.is_available() and not (args.load_in_8bit or args.load_in_4bit),
        remove_unused_columns=False,
    )

    print("\nâš™ï¸ í•™ìŠµ ì„¤ì •:")
    print(f"  - í•™ìŠµë¥ : {args.learning_rate}")
    print(f"  - ì—í¬í¬ ìˆ˜: {args.num_train_epochs}")
    print(f"  - ë°°ì¹˜ í¬ê¸°: {args.per_device_train_batch_size}")
    print(f"  - ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print(
        f"  - fp16 ì‚¬ìš©: {torch.cuda.is_available() and not (args.load_in_8bit or args.load_in_4bit)}"
    )

    # Trainer ì´ˆê¸°í™”
    print("\nğŸš€ Trainer ì´ˆê¸°í™” ë° í•™ìŠµ ì‹œì‘...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer,
    )

    # í•™ìŠµ ì‹¤í–‰
    trainer.train()

    # ëª¨ë¸ ì €ì¥
    final_output_dir = os.path.join(args.output_dir, "final_model")
    os.makedirs(final_output_dir, exist_ok=True)
    model.save_pretrained(final_output_dir)
    tokenizer.save_pretrained(final_output_dir)

    print(f"\nâœ… íŒŒì¸íŠœë‹ ì™„ë£Œ! ëª¨ë¸ì´ {final_output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    return model, tokenizer


def test_fine_tuned_model(model, tokenizer):
    """
    íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ê°„ë‹¨íˆ í…ŒìŠ¤íŠ¸
    """
    print("\nğŸ§ª íŒŒì¸íŠœë‹ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸:")

    test_prompts = [
        "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•œêµ­ ì—­ì‚¬ì— ëŒ€í•´ ê°„ëµíˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "íŒŒì´ì¬ìœ¼ë¡œ 'Hello, World!'ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
    ]

    for prompt in test_prompts:
        formatted_prompt = f"<|user|>\n{prompt}\n<|assistant|>\n"
        inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

        print(f"\nğŸ“ ì§ˆë¬¸: {prompt}")
        print("ğŸ§  ëª¨ë¸ ì‘ë‹µ ìƒì„± ì¤‘...")

        with torch.no_grad():
            outputs = model.generate(
                inputs["input_ids"],
                max_length=200,
                temperature=0.7,
                do_sample=True,
                top_p=0.9,
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"ğŸ¤– ì‘ë‹µ: {response}")


def main():
    """
    ë©”ì¸ í•¨ìˆ˜
    """
    parser = HfArgumentParser(FinetuningArguments)
    args = parser.parse_args_into_dataclasses()[0]

    print("\nğŸŒŸ LLaMA ëª¨ë¸ íŒŒì¸íŠœë‹ ì‹¤ìŠµ ğŸŒŸ")
    print("=" * 50)

    # ë””ë°”ì´ìŠ¤ í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ–¥ï¸ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
    if device == "cuda":
        print(f"  - GPU: {torch.cuda.get_device_name(0)}")
        print(
            f"  - ê°€ìš© ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB"
        )

    # íŒŒì¸íŠœë‹ ì‹¤í–‰
    model, tokenizer = fine_tune_model(args)

    # ëª¨ë¸ í…ŒìŠ¤íŠ¸
    test_fine_tuned_model(model, tokenizer)

    print("\nğŸ‰ ëª¨ë“  ê³¼ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")


if __name__ == "__main__":
    main()
