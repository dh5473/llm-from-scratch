from transformers import (
    Trainer,
    TrainingArguments,
    AutoModelForCausalLM,
    AutoTokenizer,
    DataCollatorForLanguageModeling,
)
import torch
from datasets import Dataset
import numpy as np
import matplotlib.pyplot as plt
from IPython.display import display, Markdown
import os
import time


def explain_trainer_class():
    """
    Trainer í´ë˜ìŠ¤ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ì„¤ëª…í•˜ê³  ì‹œê°í™”í•©ë‹ˆë‹¤.
    """
    print("ğŸ“š Hugging Face Trainer í´ë˜ìŠ¤ ë™ì‘ ì›ë¦¬ ì„¤ëª… ğŸ“š")
    print("=" * 70)

    # Trainer í´ë˜ìŠ¤ì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ ì„¤ëª…
    print("\n1ï¸âƒ£ Trainer í´ë˜ìŠ¤ì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸")
    print("-" * 50)

    components = {
        "model": "í•™ìŠµí•  ëª¨ë¸",
        "args": "TrainingArguments ê°ì²´ë¡œ í•™ìŠµ ì„¤ì • ì§€ì •",
        "data_collator": "ë°°ì¹˜ ìƒì„± ì‹œ ê° ìƒ˜í”Œì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜",
        "train_dataset": "í•™ìŠµ ë°ì´í„°ì…‹",
        "eval_dataset": "í‰ê°€ ë°ì´í„°ì…‹ (ì„ íƒ ì‚¬í•­)",
        "tokenizer": "í† í°í™”ë¥¼ ìœ„í•œ í† í¬ë‚˜ì´ì € (ì„ íƒ ì‚¬í•­)",
        "compute_metrics": "í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° í•¨ìˆ˜ (ì„ íƒ ì‚¬í•­)",
        "optimizers": "ìµœì í™”ê¸°ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ (ì„ íƒ ì‚¬í•­)",
    }

    for component, description in components.items():
        print(f"  â€¢ {component}: {description}")

    # í•™ìŠµ ê³¼ì • ì„¤ëª…
    print("\n2ï¸âƒ£ Trainer í•™ìŠµ ì›Œí¬í”Œë¡œìš°")
    print("-" * 50)

    workflow_steps = [
        "ëª¨ë¸, ë°ì´í„°ì…‹, í•™ìŠµ ì¸ì ì´ˆê¸°í™”",
        "ìµœì í™”ê¸° ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •",
        "ë°ì´í„° ë¡œë” ìƒì„± ë° ë°°ì¹˜ ìƒ˜í”Œë§ ì¤€ë¹„",
        "í•™ìŠµ ë£¨í”„ ì‹œì‘",
        "ë°°ì¹˜ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬",
        "ëª¨ë¸ ìˆœì „íŒŒ (forward pass) ë° ì†ì‹¤ ê³„ì‚°",
        "ì—­ì „íŒŒ (backward pass) ë° ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°",
        "ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  (ì„¤ì •ëœ ê²½ìš°)",
        "ì˜µí‹°ë§ˆì´ì € ìŠ¤í… ë° í•™ìŠµë¥  ì—…ë°ì´íŠ¸",
        "ë¡œê¹… ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥ (ì„¤ì •ëœ ê°„ê²©ì— ë”°ë¼)",
        "í‰ê°€ ë£¨í”„ ì‹¤í–‰ (ì„¤ì •ëœ ê²½ìš°)",
        "í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ëª¨ë¸ ì €ì¥",
    ]

    for i, step in enumerate(workflow_steps):
        print(f"  {i+1}. {step}")

    # ê°„ë‹¨í•œ Trainer ì‚¬ìš© ì˜ˆì œ ì‹œì—°
    print("\n3ï¸âƒ£ ê°„ë‹¨í•œ Trainer ì‚¬ìš© ì˜ˆì œ")
    print("-" * 50)

    # ë°ì´í„° ìƒ˜í”Œ ìƒì„±
    print("  â€¢ ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„± ì¤‘...")

    texts = [
        "ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ í•™ìŠµëŠ¥ë ¥ê³¼ ì¶”ë¡ ëŠ¥ë ¥, ì§€ê°ëŠ¥ë ¥ì„ ì¸ê³µì ìœ¼ë¡œ êµ¬í˜„í•œ ì»´í“¨í„° ì‹œìŠ¤í…œì…ë‹ˆë‹¤.",
        "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.",
        "ë”¥ëŸ¬ë‹ì€ ì—¬ëŸ¬ ì¸µì˜ ì¸ê³µì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.",
        "ìì—°ì–´ ì²˜ë¦¬ëŠ” ì¸ê°„ì˜ ì–¸ì–´ë¥¼ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.",
        "LLM(Large Language Model)ì€ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ë¡œ, í…ìŠ¤íŠ¸ ìƒì„± ë° ì´í•´ ëŠ¥ë ¥ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.",
    ]

    dataset = Dataset.from_dict({"text": texts})

    # í† í¬ë‚˜ì´ì € ì„¤ì •
    print("  â€¢ í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ì¤‘...")

    model_name = "meta-llama/Llama-3.2-1B"  # ì‹¤ì œ ì‹¤í–‰ ì‹œì—ëŠ” ì‘ì€ ëª¨ë¸ë¡œ ëŒ€ì²´ ê°€ëŠ¥

    try:
        # ì‹¤ì œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì—†ì´ ì„¤ëª…ë§Œ
        print(
            f"    (ì°¸ê³ : ì‹¤ì œ ëª¨ë¸({model_name})ì€ ë‹¤ìš´ë¡œë“œí•˜ì§€ ì•Šê³  ì„¤ëª…ë§Œ ì œê³µí•©ë‹ˆë‹¤)"
        )

        # ê°€ìƒì˜ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë™ì‘ ì„¤ëª…
        print("  â€¢ í† í¬ë‚˜ì´ì €ê°€ í…ìŠ¤íŠ¸ë¥¼ í† í°ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •:")
        sample_text = "ì¸ê³µì§€ëŠ¥ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤."
        print(f"    ì›ë³¸ í…ìŠ¤íŠ¸: '{sample_text}'")
        print(
            f"    í† í°í™” ê²°ê³¼ (ì˜ˆì‹œ): ['ì¸ê³µ', 'ì§€ëŠ¥', 'ëª¨ë¸', 'ì„', 'í•™ìŠµ', 'í•©ë‹ˆë‹¤', '.']"
        )
        print(f"    í† í° ID (ì˜ˆì‹œ): [14235, 9876, 2468, 1357, 8642, 5791, 9]")

    except Exception as e:
        print(f"    ëª¨ë¸ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        print("    ì„¤ëª…ì„ ìœ„í•œ ì˜ˆì‹œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")

    # TrainingArguments ì„¤ëª…
    print("\n4ï¸âƒ£ TrainingArguments ì£¼ìš” íŒŒë¼ë¯¸í„°")
    print("-" * 50)

    training_args_params = {
        "output_dir": "ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ê²½ë¡œ",
        "num_train_epochs": "í•™ìŠµ ì—í¬í¬ ìˆ˜",
        "per_device_train_batch_size": "ë””ë°”ì´ìŠ¤ë‹¹ í•™ìŠµ ë°°ì¹˜ í¬ê¸°",
        "per_device_eval_batch_size": "ë””ë°”ì´ìŠ¤ë‹¹ í‰ê°€ ë°°ì¹˜ í¬ê¸°",
        "learning_rate": "ì´ˆê¸° í•™ìŠµë¥ ",
        "weight_decay": "ê°€ì¤‘ì¹˜ ê°ì‡  ë¹„ìœ¨",
        "logging_dir": "ë¡œê·¸ ì €ì¥ ë””ë ‰í† ë¦¬",
        "logging_steps": "ë¡œê¹… ê°„ê²© (ìŠ¤í… ë‹¨ìœ„)",
        "save_steps": "ëª¨ë¸ ì €ì¥ ê°„ê²© (ìŠ¤í… ë‹¨ìœ„)",
        "save_total_limit": "ì €ì¥í•  ìµœëŒ€ ì²´í¬í¬ì¸íŠ¸ ìˆ˜",
        "evaluation_strategy": "í‰ê°€ ì „ëµ (no, steps, epoch)",
        "gradient_accumulation_steps": "ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ìŠ¤í… ìˆ˜",
        "fp16": "16ë¹„íŠ¸ ë¶€ë™ì†Œìˆ˜ì  í•™ìŠµ í™œì„±í™”",
        "warmup_steps/warmup_ratio": "ì›œì—… ìŠ¤í… ìˆ˜ ë˜ëŠ” ë¹„ìœ¨",
    }

    for param, desc in training_args_params.items():
        print(f"  â€¢ {param}: {desc}")

    # ë°ì´í„° ì½œë ˆì´í„° ì„¤ëª…
    print("\n5ï¸âƒ£ DataCollatorì˜ ì—­í• ")
    print("-" * 50)

    print("  â€¢ ë°ì´í„° ì½œë ˆì´í„°ëŠ” ê°œë³„ ìƒ˜í”Œì„ ë°°ì¹˜ë¡œ ê²°í•©í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.")
    print("  â€¢ DataCollatorForLanguageModelingì˜ ì£¼ìš” ê¸°ëŠ¥:")
    print("    1. ë‹¤ì–‘í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ë¥¼ íŒ¨ë”©í•˜ì—¬ ë™ì¼í•œ ê¸¸ì´ë¡œ ë§Œë“¦")
    print("    2. ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ ëª¨ë¸ë§(MLM)ì—ì„œ ì¼ë¶€ í† í°ì„ ë§ˆìŠ¤í‚¹")
    print("    3. ì¸ê³¼ì  ì–¸ì–´ ëª¨ë¸ë§(CLM)ì—ì„œ ì…ë ¥ì„ ë ˆì´ë¸”ë¡œ ì‚¬ìš©")
    print("    4. ë°°ì¹˜ ë‚´ ìƒ˜í”Œë“¤ì„ í…ì„œë¡œ ë³€í™˜")

    # í•™ìŠµ ë£¨í”„ ì‹œê°í™”
    print("\n6ï¸âƒ£ í•™ìŠµ ë£¨í”„ ì‹œê°í™”")
    print("-" * 50)

    # ê°€ìƒì˜ í•™ìŠµ ë°ì´í„° ìƒì„±
    epochs = 3
    steps_per_epoch = 5
    loss_values = []

    # ë‹¨ìˆœí•œ ê°ì†Œ íŒ¨í„´ì˜ ì†ì‹¤ ê°’ ìƒì„±
    for epoch in range(epochs):
        for step in range(steps_per_epoch):
            # ë…¸ì´ì¦ˆê°€ ìˆëŠ” ê°ì†Œ íŒ¨í„´
            loss = (
                2.5
                - (epoch * steps_per_epoch + step) / (epochs * steps_per_epoch) * 2.0
            )
            loss += np.random.normal(0, 0.1)  # ì•½ê°„ì˜ ë…¸ì´ì¦ˆ ì¶”ê°€
            loss_values.append(max(0.1, loss))  # ìµœì†Œê°’ ë³´ì¥

    # í•™ìŠµ ë£¨í”„ ì‹œê°í™” (ASCII ì•„íŠ¸)
    print("  â€¢ í•™ìŠµ ë£¨í”„ ì§„í–‰ ê³¼ì • (ì˜ˆì‹œ):")
    print("  " + "-" * 40)
    print("  |  ì—í¬í¬  |  ìŠ¤í…  |  ì†ì‹¤  |  í•™ìŠµë¥   |")
    print("  " + "-" * 40)

    for epoch in range(epochs):
        for step in range(steps_per_epoch):
            idx = epoch * steps_per_epoch + step
            loss = loss_values[idx]
            lr = 5e-5 * (1 - idx / (epochs * steps_per_epoch))  # í•™ìŠµë¥  ê°ì†Œ
            print(f"  |    {epoch+1}     |   {step+1}    | {loss:.4f} | {lr:.6f} |")

    print("  " + "-" * 40)

    # ì†ì‹¤ ë³€í™” ì‹œê°í™”ë¥¼ ìœ„í•œ í…ìŠ¤íŠ¸ ê¸°ë°˜ ê·¸ë˜í”„
    print("\n  â€¢ ì†ì‹¤ ë³€í™” (í…ìŠ¤íŠ¸ ì‹œê°í™”):")
    max_loss = max(loss_values)
    min_loss = min(loss_values)
    scale = 20  # ê·¸ë˜í”„ ë„ˆë¹„

    for epoch in range(epochs):
        print(f"  ì—í¬í¬ {epoch+1}: ", end="")
        for step in range(steps_per_epoch):
            idx = epoch * steps_per_epoch + step
            loss = loss_values[idx]
            # ì†ì‹¤ ê°’ì„ 0-scale ë²”ìœ„ë¡œ ì •ê·œí™”í•˜ì—¬ '#' ë¬¸ì ê°œìˆ˜ ê²°ì •
            bars = (
                int((loss - min_loss) / (max_loss - min_loss) * scale)
                if max_loss > min_loss
                else 0
            )
            print("#" * bars + " " * (scale - bars) + f" {loss:.4f}", end="  ")
        print()

    # Trainerì˜ ì£¼ìš” ë©”ì„œë“œ ì„¤ëª…
    print("\n7ï¸âƒ£ Trainer í´ë˜ìŠ¤ì˜ ì£¼ìš” ë©”ì„œë“œ")
    print("-" * 50)

    methods = {
        "train()": "í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.",
        "evaluate()": "í‰ê°€ ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì„ í‰ê°€í•©ë‹ˆë‹¤.",
        "predict()": "í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        "save_model()": "ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.",
        "log()": "ì§€ì •ëœ ë¡œê·¸ ê°’ì„ ê¸°ë¡í•©ë‹ˆë‹¤.",
        "create_optimizer_and_scheduler()": "ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.",
        "compute_loss()": "ëª¨ë¸ì˜ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.",
        "training_step()": "ë‹¨ì¼ í•™ìŠµ ìŠ¤í…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        "prediction_step()": "ë‹¨ì¼ ì˜ˆì¸¡ ìŠ¤í…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
    }

    for method, desc in methods.items():
        print(f"  â€¢ {method}: {desc}")

    # í•™ìŠµ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ë°©ë²•
    print("\n8ï¸âƒ£ í•™ìŠµ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§")
    print("-" * 50)

    monitoring_tools = {
        "ë¡œê¹…": "logging_steps ì¸ìë¥¼ í†µí•´ ë¡œê¹… ë¹ˆë„ ì„¤ì •",
        "TensorBoard": "--report_to tensorboard ì˜µì…˜ìœ¼ë¡œ ì‹œê°í™” ë„êµ¬ í™œì„±í™”",
        "Weights & Biases": "--report_to wandb ì˜µì…˜ìœ¼ë¡œ W&B í†µí•©",
        "ì»¤ìŠ¤í…€ ì½œë°±": "TrainerCallback í´ë˜ìŠ¤ë¥¼ ìƒì†í•˜ì—¬A ë§ì¶¤í˜• ì½œë°± êµ¬í˜„",
        "ì§„í–‰ ìƒí™© í‘œì‹œ": "disable_tqdm=False ì„¤ì •ìœ¼ë¡œ ì§„í–‰ í‘œì‹œì¤„ í™œì„±í™”",
    }

    for tool, desc in monitoring_tools.items():
        print(f"  â€¢ {tool}: {desc}")

    # ë§ì¶¤í˜• Trainer í•˜ìœ„ í´ë˜ìŠ¤ ì˜ˆì‹œ
    print("\n9ï¸âƒ£ ë§ì¶¤í˜• Trainer í•˜ìœ„ í´ë˜ìŠ¤ ì˜ˆì‹œ")
    print("-" * 50)

    custom_trainer_code = """class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False):
        # ê¸°ë³¸ êµ¬í˜„ ëŒ€ì‹  ì»¤ìŠ¤í…€ ì†ì‹¤ ê³„ì‚° ë¡œì§ ì¶”ê°€
        labels = inputs.pop("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        
        # ì»¤ìŠ¤í…€ ì†ì‹¤ í•¨ìˆ˜ (ì˜ˆ: ë ˆì´ë¸” ìŠ¤ë¬´ë”© ì ìš©)
        loss_fct = torch.nn.CrossEntropyLoss(label_smoothing=0.1)
        loss = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss
    
    def training_step(self, model, inputs):
        # ì»¤ìŠ¤í…€ í•™ìŠµ ìŠ¤í… êµ¬í˜„
        model.train()
        inputs = self._prepare_inputs(inputs)
        
        # ì¼ë°˜ì ì¸ ì†ì‹¤ ê³„ì‚°
        loss = self.compute_loss(model, inputs)
        
        # ì¶”ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ì˜ˆ: í¼í”Œë ‰ì„œí‹°)
        perplexity = torch.exp(loss)
        
        # ë¡œê¹…
        self.log({"loss": loss.detach().item(), "perplexity": perplexity.detach().item()})
        
        # ìŠ¤ì¼€ì¼ë§ ë° ì—­ì „íŒŒ
        loss = self.accelerator.backward(loss)
        
        return loss.detach()"""

    print(f"```python\n{custom_trainer_code}\n```")

    # ê²°ë¡ 
    print("\nğŸ”Ÿ Trainer í´ë˜ìŠ¤ ìš”ì•½")
    print("-" * 50)

    print(
        "  â€¢ Trainer í´ë˜ìŠ¤ëŠ” Hugging Face Transformersì—ì„œ ëª¨ë¸ í•™ìŠµì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤."
    )
    print("  â€¢ ì£¼ìš” ì¥ì :")
    print("    - ë³µì¡í•œ í•™ìŠµ ë£¨í”„ë¥¼ ì¶”ìƒí™”í•˜ì—¬ ì½”ë“œ ì‘ì„±ëŸ‰ ê°ì†Œ")
    print("    - ë¶„ì‚° í•™ìŠµ, í˜¼í•© ì •ë°€ë„ í•™ìŠµ ë“± ë‹¤ì–‘í•œ ê¸°ëŠ¥ ë‚´ì¥")
    print("    - í‰ê°€, ë¡œê¹…, ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ë“± í¸ì˜ ê¸°ëŠ¥ ì œê³µ")
    print("    - ì»¤ìŠ¤í„°ë§ˆì´ì§•ì„ í†µí•´ íŠ¹ìˆ˜í•œ ìš”êµ¬ì‚¬í•­ êµ¬í˜„ ê°€ëŠ¥")
    print("  â€¢ í•™ìŠµ ë‹¨ê³„ ìš”ì•½:")
    print("    1. ëª¨ë¸, ë°ì´í„°ì…‹, í•™ìŠµ ì¸ì ì´ˆê¸°í™”")
    print("    2. ì˜µí‹°ë§ˆì´ì €ì™€ ìŠ¤ì¼€ì¤„ëŸ¬ êµ¬ì„±")
    print("    3. ë°°ì¹˜ ì²˜ë¦¬ ë° í•™ìŠµ ë£¨í”„ ì‹¤í–‰")
    print("    4. ì •ê¸°ì ì¸ ë¡œê¹… ë° ì²´í¬í¬ì¸íŠ¸ ì €ì¥")
    print("    5. í‰ê°€ ë° ìµœì¢… ëª¨ë¸ ì €ì¥")


if __name__ == "__main__":
    explain_trainer_class()
