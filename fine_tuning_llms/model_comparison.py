import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import pandas as pd


def compare_base_instruct_models():
    """
    ë©”íƒ€ LLaMA ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸ì˜ ì°¨ì´ì ì„ ë¹„êµí•©ë‹ˆë‹¤.
    ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ë‘ ëª¨ë¸ì˜ ì‘ë‹µì„ ë¹„êµí•˜ì—¬ ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸ì´
    ì‚¬ìš©ì ì§€ì‹œì— ë” ì í•©í•˜ê²Œ ì‘ë‹µí•˜ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    """
    base_model_name = "meta-llama/Llama-3.2-1B"
    instruct_model_name = "meta-llama/Llama-3.2-1B-Instruct"

    print(
        f"â³ ë² ì´ìŠ¤ ëª¨ë¸({base_model_name})ê³¼ ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸({instruct_model_name})ì„ ë¹„êµí•©ë‹ˆë‹¤..."
    )

    # ë² ì´ìŠ¤ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    base_tokenizer = AutoTokenizer.from_pretrained(base_model_name)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name, torch_dtype=torch.float16, device_map="auto"
    )

    # ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
    instruct_tokenizer = AutoTokenizer.from_pretrained(instruct_model_name)
    instruct_model = AutoModelForCausalLM.from_pretrained(
        instruct_model_name, torch_dtype=torch.float16, device_map="auto"
    )

    # ëª¨ë¸ êµ¬ì„± ë¹„êµ
    print("\nğŸ“Š ëª¨ë¸ êµ¬ì„± ë¹„êµ:")
    base_config = base_model.config
    instruct_config = instruct_model.config

    # ë‘ ëª¨ë¸ì˜ ì£¼ìš” ì°¨ì´ì  ì¶œë ¥
    print(f"  - ë² ì´ìŠ¤ ëª¨ë¸ ì–´íœ˜ í¬ê¸°: {base_config.vocab_size}")
    print(f"  - ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸ ì–´íœ˜ í¬ê¸°: {instruct_config.vocab_size}")

    # ê¸°íƒ€ ì£¼ìš” íŒŒë¼ë¯¸í„° ë¹„êµ
    for param in ["hidden_size", "num_attention_heads", "num_hidden_layers"]:
        if hasattr(base_config, param) and hasattr(instruct_config, param):
            print(
                f"  - {param}: ë² ì´ìŠ¤={getattr(base_config, param)}, ì¸ìŠ¤íŠ¸ëŸ­íŠ¸={getattr(instruct_config, param)}"
            )

    # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¡œ ë‘ ëª¨ë¸ì˜ ì¶œë ¥ ë¹„êµ
    test_prompts = [
        "ì¸ê³µì§€ëŠ¥ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
        "í•œêµ­ ì—­ì‚¬ì— ëŒ€í•´ ê°„ëµíˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
        "íŒŒì´ì¬ìœ¼ë¡œ 'Hello, World!'ë¥¼ ì¶œë ¥í•˜ëŠ” ì½”ë“œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.",
    ]

    print("\nğŸ”„ ë‘ ëª¨ë¸ì˜ ì‘ë‹µ ë¹„êµ:")

    results = []

    for prompt in test_prompts:
        # ë² ì´ìŠ¤ ëª¨ë¸ ì‘ë‹µ ìƒì„±
        base_inputs = base_tokenizer(prompt, return_tensors="pt").to(base_model.device)
        base_output = base_model.generate(
            base_inputs["input_ids"], max_length=100, do_sample=True, temperature=0.7
        )
        base_response = base_tokenizer.decode(base_output[0], skip_special_tokens=True)

        # ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸ ì‘ë‹µ ìƒì„±
        instruct_prompt = f"<|user|>\n{prompt}\n<|assistant|>\n"
        instruct_inputs = instruct_tokenizer(instruct_prompt, return_tensors="pt").to(
            instruct_model.device
        )
        instruct_output = instruct_model.generate(
            instruct_inputs["input_ids"],
            max_length=100,
            do_sample=True,
            temperature=0.7,
            attention_mask=instruct_inputs["attention_mask"],
        )
        instruct_response = instruct_tokenizer.decode(
            instruct_output[0], skip_special_tokens=True
        )

        # ê²°ê³¼ ì €ì¥
        results.append(
            {
                "í”„ë¡¬í”„íŠ¸": prompt,
                "ë² ì´ìŠ¤ ëª¨ë¸ ì‘ë‹µ": base_response,
                "ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸ ì‘ë‹µ": instruct_response,
            }
        )

        print(f"\nğŸ“ í”„ë¡¬í”„íŠ¸: {prompt}")
        print(f"ğŸ¤– ë² ì´ìŠ¤ ëª¨ë¸: {base_response[:150]}...")
        print(f"ğŸ§  ì¸ìŠ¤íŠ¸ëŸ­íŠ¸ ëª¨ë¸: {instruct_response[:150]}...")

    # ë¹„êµ ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
    df = pd.DataFrame(results)
    output_path = "model_comparison_results.csv"
    df.to_csv(output_path, index=False)
    print(f"\nâœ… ë¹„êµ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


if __name__ == "__main__":
    compare_base_instruct_models()
